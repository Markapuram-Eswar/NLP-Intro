{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b273f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553bdd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello everyone. let's learn NLP! let's start with the basics. \n",
    "I am running, jumping, and swimming in the beautiful garden. \n",
    "The cats are playing, dogs are barking, and birds are flying high above.\n",
    "I love reading books, writing stories, and learning new things every day.\n",
    "The students are studying, teachers are teaching, and parents are working hard.\n",
    "Beautiful flowers are blooming, trees are growing, and rivers are flowing peacefully.\n",
    "I enjoy walking, running, cycling, and hiking in the mountains.\n",
    "The children are playing, laughing, and having fun at the playground.\n",
    "I have been working, studying, and improving my skills continuously.\n",
    "The weather is changing, seasons are shifting, and nature is evolving.\n",
    "I love cooking, baking, and preparing delicious meals for my family.\n",
    "The artists are painting, musicians are playing, and dancers are performing gracefully.\n",
    "I enjoy traveling, exploring, and discovering new places around the world.\n",
    "The scientists are researching, experimenting, and discovering amazing things.\n",
    "I love gardening, planting, and watching my plants growing beautifully.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b646b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439a02af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.', \"let's learn NLP!\", \"let's start with the basics\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6e61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdfd71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'learn',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'start',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basics']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232a6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f6e8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone.',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'learn',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'start',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basics']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6f9af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " 'let',\n",
       " 's',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " 'let',\n",
       " 's',\n",
       " 'start',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basics']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3227c474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'start',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basics']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90dcabe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " \"let's\",\n",
       " 'learn',\n",
       " 'NLP',\n",
       " '!',\n",
       " \"let's\",\n",
       " 'start',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basics']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f98787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.', \"let's learn NLP!\", \"let's start with the basics\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktTokenizer\n",
    "tokenizer = PunktTokenizer()\n",
    "tokenizer.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8f820c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone.',\n",
       " \"let's\",\n",
       " 'learn',\n",
       " 'NLP!',\n",
       " \"let's\",\n",
       " 'start',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basics']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenizer.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0890eb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'start',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basics']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc7368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the list: 99\n",
      "First 20 words: ['apples', 'running', 'jumping', 'swimming', 'playing', 'barking', 'flying', 'reading', 'writing', 'learning', 'studying', 'teaching', 'working', 'blooming', 'growing', 'flowing', 'walking', 'cycling', 'hiking', 'laughing']\n"
     ]
    }
   ],
   "source": [
    "# List of words for stemming demonstration\n",
    "words = [\n",
    "    \"apples\", \"running\", \"jumping\", \"swimming\", \"playing\", \"barking\", \"flying\",\n",
    "    \"reading\", \"writing\", \"learning\", \"studying\", \"teaching\", \"working\", \n",
    "    \"blooming\", \"growing\", \"flowing\", \"walking\", \"cycling\", \"hiking\",\n",
    "    \"laughing\", \"changing\", \"shifting\", \"evolving\", \"cooking\", \"baking\",\n",
    "    \"preparing\", \"painting\", \"performing\", \"traveling\", \"exploring\", \n",
    "    \"discovering\", \"researching\", \"experimenting\", \"gardening\", \"planting\",\n",
    "    \"watching\", \"improved\", \"changed\", \"shifted\", \"evolved\", \"cats\", \"dogs\",\n",
    "    \"birds\", \"flowers\", \"trees\", \"rivers\", \"children\", \"seasons\", \"artists\",\n",
    "    \"musicians\", \"dancers\", \"scientists\", \"plants\", \"teachers\", \"parents\",\n",
    "    \"beautifully\", \"peacefully\", \"gracefully\", \"continuously\", \"quickly\",\n",
    "    \"slowly\", \"carefully\", \"happily\", \"sadly\", \"angrily\", \"quietly\",\n",
    "    \"loudly\", \"brightly\", \"darkly\", \"clearly\", \"vaguely\", \"easily\",\n",
    "    \"difficultly\", \"simply\", \"complexly\", \"naturally\", \"artificially\",\n",
    "    \"automatically\", \"manually\", \"digitally\", \"physically\", \"mentally\",\n",
    "    \"emotionally\", \"spiritually\", \"financially\", \"economically\", \"politically\",\n",
    "    \"socially\", \"culturally\", \"historically\", \"geographically\", \"scientifically\",\n",
    "    \"technologically\", \"mathematically\", \"statistically\", \"psychologically\",\n",
    "    \"philosophically\", \"theoretically\", \"practically\"\n",
    "]\n",
    "\n",
    "print(f\"Total words in the list: {len(words)}\")\n",
    "print(\"First 20 words:\", words[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c802963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1292d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e7e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "jump\n",
      "swim\n",
      "play\n",
      "bark\n",
      "fli\n"
     ]
    }
   ],
   "source": [
    "print(port_stem.stem(\"running\"))\n",
    "print(port_stem.stem(\"jumping\"))\n",
    "print(port_stem.stem(\"swimming\"))\n",
    "print(port_stem.stem(\"playing\"))\n",
    "print(port_stem.stem(\"barking\"))\n",
    "print(port_stem.stem(\"flying\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5f3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "jump\n",
      "swim\n",
      "play\n",
      "bark\n",
      "fly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lanc_stem = LancasterStemmer()\n",
    "print(lanc_stem.stem(\"running\"))\n",
    "print(lanc_stem.stem(\"jumping\"))\n",
    "print(lanc_stem.stem(\"swimming\"))\n",
    "print(lanc_stem.stem(\"playing\"))\n",
    "print(lanc_stem.stem(\"barking\"))\n",
    "print(lanc_stem.stem(\"flying\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3006887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "jump\n",
      "swim\n",
      "play\n",
      "bark\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer(language=\"english\")\n",
    "print(snow_stem.stem(\"running\"))\n",
    "print(snow_stem.stem(\"jumping\"))\n",
    "print(snow_stem.stem(\"swimming\"))\n",
    "print(snow_stem.stem(\"playing\"))\n",
    "print(snow_stem.stem(\"barking\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168e6318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runn\n",
      "jump\n",
      "swimm\n",
      "play\n",
      "ingiwg\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing$')\n",
    "print(stemmer.stem(\"running\"))\n",
    "print(stemmer.stem(\"jumping\"))\n",
    "print(stemmer.stem(\"swimming\"))\n",
    "print(stemmer.stem(\"playing\"))\n",
    "print(stemmer.stem(\"ingiwg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb07b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "jumping\n",
      "swimming\n",
      "playing\n",
      "barking\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\"))\n",
    "print(lemmatizer.lemmatize(\"jumping\"))\n",
    "print(lemmatizer.lemmatize(\"swimming\"))\n",
    "print(lemmatizer.lemmatize(\"playing\"))\n",
    "print(lemmatizer.lemmatize(\"barking\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b244bc",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) Implementation\n",
    "\n",
    "Bag of Words is a simple but effective text representation technique that converts text into numerical vectors. It creates a vocabulary from all unique words in the corpus and represents each document as a vector where each dimension corresponds to a word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8dcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample documents for Bag of Words demonstration\n",
    "documents = [\n",
    "    \"I love machine learning and artificial intelligence\",\n",
    "    \"Machine learning is fascinating and powerful\",\n",
    "    \"I enjoy studying natural language processing\",\n",
    "    \"Natural language processing helps computers understand text\",\n",
    "    \"Artificial intelligence and machine learning are related fields\",\n",
    "    \"I love working with text data and algorithms\"\n",
    "]\n",
    "\n",
    "print(\"Sample Documents:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fda040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual Bag of Words Implementation\n",
    "def manual_bag_of_words(documents):\n",
    "    \"\"\"\n",
    "    Manual implementation of Bag of Words\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize and create vocabulary\n",
    "    all_words = []\n",
    "    for doc in documents:\n",
    "        words = doc.lower().split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Remove punctuation and get unique words\n",
    "    vocabulary = list(set(all_words))\n",
    "    vocabulary.sort()  # Sort for consistent ordering\n",
    "    \n",
    "    print(f\"Vocabulary ({len(vocabulary)} unique words):\")\n",
    "    print(vocabulary)\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Create Bag of Words vectors\n",
    "    bow_vectors = []\n",
    "    for doc in documents:\n",
    "        words = doc.lower().split()\n",
    "        word_count = Counter(words)\n",
    "        vector = [word_count.get(word, 0) for word in vocabulary]\n",
    "        bow_vectors.append(vector)\n",
    "    \n",
    "    return vocabulary, bow_vectors\n",
    "\n",
    "vocabulary, bow_vectors = manual_bag_of_words(documents)\n",
    "\n",
    "# Display the results\n",
    "print(\"Bag of Words Vectors:\")\n",
    "for i, vector in enumerate(bow_vectors, 1):\n",
    "    print(f\"Document {i}: {vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_vectors, columns=vocabulary, index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
    "print(\"Bag of Words Matrix:\")\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11874a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using Scikit-learn CountVectorizer\n",
    "print(\"=\" * 60)\n",
    "print(\"Using Scikit-learn CountVectorizer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Vocabulary ({len(feature_names)} unique words):\")\n",
    "print(feature_names)\n",
    "print()\n",
    "\n",
    "# Convert to dense matrix for better visualization\n",
    "bow_dense = bow_matrix.toarray()\n",
    "\n",
    "# Create DataFrame\n",
    "sklearn_bow_df = pd.DataFrame(bow_dense, columns=feature_names, index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
    "print(\"Scikit-learn Bag of Words Matrix:\")\n",
    "print(sklearn_bow_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CountVectorizer with different parameters\n",
    "print(\"=\" * 60)\n",
    "print(\"Advanced CountVectorizer Parameters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CountVectorizer with custom parameters\n",
    "advanced_vectorizer = CountVectorizer(\n",
    "    lowercase=True,           # Convert to lowercase\n",
    "    stop_words='english',     # Remove English stop words\n",
    "    ngram_range=(1, 2),       # Use unigrams and bigrams\n",
    "    max_features=20,          # Limit to top 20 features\n",
    "    min_df=1,                 # Minimum document frequency\n",
    "    max_df=1.0                # Maximum document frequency\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "advanced_bow = advanced_vectorizer.fit_transform(documents)\n",
    "advanced_features = advanced_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Advanced Vocabulary ({len(advanced_features)} features):\")\n",
    "print(advanced_features)\n",
    "print()\n",
    "\n",
    "# Create DataFrame\n",
    "advanced_df = pd.DataFrame(\n",
    "    advanced_bow.toarray(), \n",
    "    columns=advanced_features, \n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "print(\"Advanced Bag of Words Matrix (with n-grams and stop words removed):\")\n",
    "print(advanced_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "print(\"=\" * 60)\n",
    "print(\"TF-IDF Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=15\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF Vocabulary ({len(tfidf_features)} features):\")\n",
    "print(tfidf_features)\n",
    "print()\n",
    "\n",
    "# Create DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=tfidf_features, \n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df.round(3))  # Round to 3 decimal places for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Bag of Words\n",
    "print(\"=\" * 60)\n",
    "print(\"Bag of Words Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a heatmap for the basic BoW matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(sklearn_bow_df, annot=True, cmap='Blues', fmt='d', cbar_kws={'label': 'Word Count'})\n",
    "plt.title('Bag of Words Matrix Heatmap')\n",
    "plt.xlabel('Words (Features)')\n",
    "plt.ylabel('Documents')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word frequency analysis\n",
    "print(\"\\nWord Frequency Analysis:\")\n",
    "word_frequencies = sklearn_bow_df.sum().sort_values(ascending=False)\n",
    "print(word_frequencies.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Text Classification with Bag of Words\n",
    "print(\"=\" * 60)\n",
    "print(\"Practical Example: Text Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create a simple dataset for classification\n",
    "texts = [\n",
    "    \"I love this movie, it's amazing!\",\n",
    "    \"This film is terrible and boring\",\n",
    "    \"Great acting and wonderful story\",\n",
    "    \"Worst movie I've ever seen\",\n",
    "    \"Fantastic plot with excellent characters\",\n",
    "    \"Boring and waste of time\",\n",
    "    \"Outstanding performance by actors\",\n",
    "    \"Disappointing and poorly made\",\n",
    "    \"Brilliant cinematography and direction\",\n",
    "    \"Awful script and bad acting\"\n",
    "]\n",
    "\n",
    "labels = ['positive', 'negative', 'positive', 'negative', 'positive', \n",
    "          'negative', 'positive', 'negative', 'positive', 'negative']\n",
    "\n",
    "print(\"Sample texts and labels:\")\n",
    "for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "    print(f\"{i+1}. [{label.upper()}] {text}\")\n",
    "\n",
    "# Create Bag of Words representation\n",
    "classifier_vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "X = classifier_vectorizer.fit_transform(texts)\n",
    "y = labels\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {classifier_vectorizer.get_feature_names_out()}\")\n",
    "\n",
    "# Train a simple classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "print(f\"\\nTest predictions: {y_pred}\")\n",
    "print(f\"Actual labels: {y_test}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63bb76",
   "metadata": {},
   "source": [
    "## Key Concepts and Advantages/Disadvantages\n",
    "\n",
    "### Bag of Words Advantages:\n",
    "1. **Simple and Intuitive**: Easy to understand and implement\n",
    "2. **Language Independent**: Works with any language\n",
    "3. **Fast Processing**: Efficient for large datasets\n",
    "4. **Good Baseline**: Often used as a starting point for text analysis\n",
    "\n",
    "### Bag of Words Disadvantages:\n",
    "1. **Loss of Word Order**: Ignores the sequence of words\n",
    "2. **Sparse Matrix**: Creates high-dimensional sparse vectors\n",
    "3. **No Semantic Understanding**: Doesn't capture word meanings or relationships\n",
    "4. **Vocabulary Size**: Can become very large with diverse text\n",
    "\n",
    "### When to Use Bag of Words:\n",
    "- Text classification tasks\n",
    "- Document similarity analysis\n",
    "- As a baseline for more advanced methods\n",
    "- When word order is not critical\n",
    "- For quick prototyping\n",
    "\n",
    "### Advanced Alternatives:\n",
    "- **TF-IDF**: Better than simple BoW for most tasks\n",
    "- **Word Embeddings**: Word2Vec, GloVe, FastText\n",
    "- **Contextual Embeddings**: BERT, GPT, ELMo\n",
    "- **N-grams**: Capture some word order information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5b2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
